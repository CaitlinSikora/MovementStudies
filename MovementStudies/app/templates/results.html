<!-- extend base layout -->
{% extends "base.html" %}

{% block content %}
    <link href="http://vjs.zencdn.net/5.8.8/video-js.css" rel="stylesheet">
    <link rel="stylesheet" href="static/base.css" />
    <style>
        tr {
            padding: 0 40px;
        }
        th {
            padding: 10px 20px;
        }
    </style>
    <div class="contain_form">
        <h1>Human Movement Quality Study Results</h1>
        <p>The links below lead to visualizations of the results from a recent study on movement quality and body language. In this study, participants were asked to encode three out of twelve possible videos by noting start and end times for movements that they perceived to be expressive, classifying each movement with a Laban Effort (a system of established movement qualities from the discipline of dance) and an emotional word, as well as noting involved body parts. For participants, the task of encoding a video with specific times is tedious and time-consuming, so this web application was developed to aid in the process. Due to an indexing error in the application that could not be resolved in production, four out of eight motion capture animations had not been encoded by enough participants to provide reasonable results by the termination of the study, so only 8 out of 12 videos are included here.</p>
        <p>Click on a video number to see how body language was interpretted in each video:
            <table>
                <tr>
                    <th><a href="{{ url_for('segmentvis') }}?video=1">Video 1</a></th>
                    <th><a href="{{ url_for('segmentvis') }}?video=2">Video 2</a></th>
                    <th><a href="{{ url_for('segmentvis') }}?video=3">Video 3</a></th>
                    <th><a href="{{ url_for('segmentvis') }}?video=4">Video 4</a></th>
                    <th><a href="{{ url_for('segmentvis') }}?video=5">Video 5</a></th>
                    <th><a href="{{ url_for('segmentvis') }}?video=6">Video 6</a></th>
                    <th><a href="{{ url_for('segmentvis') }}?video=7">Video 7</a></th>
                    <th><a href="{{ url_for('segmentvis') }}?video=8">Video 8</a></th>
                </tr>
            </table>
        <p>The volunteers who participated in the study included: 65 people with an average age of 33.4 years, 38 women and 27 men, 31 people with average 16 years of movement training and 34 with none, and 56 people from the United States and 9 people from other countries. In order to assess the accuracy of Effort and affect classifications amongst the participants, we need to have agreed upon movements (samples) and a correct label (prediction). Because the problem of time segmentation was left open to participants, we must determine the most commonly identified segments and compare encodings that fall into those segments. We expect the precision of times identified by participants to be somewhat low. To address this issue, K-Means clustering of all segments was used to identify the most likely time segments for each video. Each of the clusters was analyzed for both Effort and affect classifications. Effort accuracy was calculated taking the mode as the correct Effort label.</p>
        <p>The visualizations above feature the original videos from the survey juxtaposed with all of the timelines of segments submitted by survey participants with start and end times marked. A composite timeline shows start and end times for all segments identified by participants as well as clusters identified by K-Means. A scatter plot of End-time vs. Start-time helps users to visualize the clusters of segments. The user can play the video to see instantaneous Effort modes and accuracies updated in real-time, along with individual participantâ€™s Effort labels, emotions, and body parts at each time. Cluster Efforts, accuracies, and spreads are also updated with video play. Average participant-encoded valence and arousal, along with standard deviation of each, are displayed for each video to allow users to better understand the reliability with which participants were able to interpret emotions from the body movements in the videos.</p>
        <p>For more information on this research, feel free to browse the <a href='/static/HumanMovementQuality.pdf'>paper</a>.</p>

        <p>If you have any questions, please email Caitlin at cas836@nyu.edu. Thanks again!</p>
    </div>
{% endblock %}